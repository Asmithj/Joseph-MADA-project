---
title: "The Impact of Malaria Episodes and Treatment Regimens on Adverse Pregnancy Outcomes in Ugandan Women"
author: "Asmith Joseph"
date: "2025-02-23"
output: html_document
---



```{r}
# Load Required Libraries

# Path management
library(here)              # For file path handling

# Data manipulation and wrangling
library(dplyr)             # Data manipulation
library(tidyverse)         # Core data science packages (includes ggplot2, dplyr, readr, etc.)
library(janitor)           # Clean column names
library(skimr)             # Data summary
library(lubridate)         # Date and time handling
library(forcats)           # Handling categorical variables

# Visualization
library(ggplot2)           # Data visualization (also part of tidyverse)
library(ggpubr)            # Publication-ready plots

# Tables and reporting
library(gtsummary)         # Summary statistics tables
library(gt)                # Table formatting
library(knitr)             # Report generation
library(kableExtra)        # Enhanced table output in markdown

# Analysis-specific packages
library(Amelia)            # Visualizing missing data
library(pwr)               # Power analysis
library(DiagrammeR)        # Diagrams and flowcharts
library(survival)          # Survival analysis
library(survminer)         # Survival plots
library(ggeffects)         # Marginal effects for regression models
library(broom)             # Tidy model outputs

# Machine learning and modeling
library(tidymodels)        # Unified modeling framework (includes recipes, parsnip, workflows, tune, etc.)
library(rsample)           # Resampling and data splitting
library(recipes)           # Preprocessing
library(parsnip)           # Model specification
library(workflows)         # Combine models and preprocessing
library(tune)              # Hyperparameter tuning
library(yardstick)         # Model performance metrics
library(dials)             # Parameter tuning grids
library(ranger)            # Random forest engine
library(xgboost)           # Boosting engine
library(themis)            # Handling class imbalance (e.g., SMOTE)

```



```{r}
# Data Import & Initial Inspection 
# load the Dataset
PROMO_Data <- read_csv(here("data", "raw-data", "PROMO_Data.csv"))


promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))
```

```{r}
colnames(promo_data_clean)
```

```{r}
names(promo_data_clean)
```

*Table 1*

```{r}
# Recode variables for baseline table presentation:
promo_data_clean <- promo_data_clean %>%
  mutate(
    # Create an age group variable (for potential subgroup analyses)
    age_group = ifelse(age_at_enrollment_years < 25, "Young", "Older"),
    
    # Recode Gravidity into categories: "1", "2–3", "≥4" (ordered chronologically)
    Gravidity_cat = case_when(
      gravidity == 1 ~ "1",
      gravidity %in% c(2, 3) ~ "2–3",
      gravidity >= 4 ~ "≥4"
    ),
    Gravidity_cat = factor(Gravidity_cat, levels = c("1", "2–3", "≥4")),
    
    # Recode Parity into categories: "0", "1–2", "≥3" (ordered chronologically)
    Parity_cat = case_when(
      parity == 0 ~ "0",
      parity %in% c(1, 2) ~ "1–2",
      parity >= 3 ~ "≥3"
    ),
    Parity_cat = factor(Parity_cat, levels = c("0", "1–2", "≥3")),
    
    # Recode Total Malaria Episodes into categories:
    # Combine 0 and 1 episodes as "1", 2-3 as "2–3", and 4 or more as "≥4"
    MalariaEpisodes_cat = case_when(
      total_malaria_episodes %in% c(0, 1) ~ "1",
      total_malaria_episodes %in% c(2, 3) ~ "2–3",
      total_malaria_episodes >= 4 ~ "≥4"
    ),
    MalariaEpisodes_cat = factor(MalariaEpisodes_cat, levels = c("1", "2–3", "≥4")),
    
    # Recode Total Malaria Episodes During Pregnancy similarly:
    MalariaEpisodesPreg_cat = case_when(
      total_malaria_episodes_during_pregnancy %in% c(0, 1) ~ "1",
      total_malaria_episodes_during_pregnancy %in% c(2, 3) ~ "2–3",
      total_malaria_episodes_during_pregnancy >= 4 ~ "≥4"
    ),
    MalariaEpisodesPreg_cat = factor(MalariaEpisodesPreg_cat, levels = c("1", "2–3", "≥4")),
    
    # Recode Preterm Births Count into categories:
    # Combine 0 and 1 as "1", and 2 as "2"
    PretermBirths_cat = case_when(
      preterm_births_count %in% c(0, 1) ~ "1",
      preterm_births_count == 2 ~ "2"
    ),
    PretermBirths_cat = factor(PretermBirths_cat, levels = c("1", "2"))
  )

# Create Table X: Baseline Characteristics Stratified by IPTp Treatment Arm
baseline_table_treatment <- promo_data_clean %>%
  dplyr::select(
    study_arm,
    `Age (years)` = age_at_enrollment_years,
    `Gestational Age (weeks)` = gestational_age_at_enrollment_weeks,
    `Maternal Education Level` = education_level,
    Gravidity = Gravidity_cat,
    Parity = Parity_cat,
    `Total Malaria Episodes` = MalariaEpisodes_cat,
    `Total Malaria Episodes During Pregnancy` = MalariaEpisodesPreg_cat,
    `Malaria Infection Rate During Pregnancy` = malaria_infection_rate_during_pregnancy,
    `Placental Malaria (Rogerson Criteria)` = placental_malaria_by_rogerson_criteria,
    `Preterm Births Count` = PretermBirths_cat,
    `Stillbirth bin` = stillbirth_bin,
    `Birthweight` = birth_weight
  ) %>%
  tbl_summary(
    by = study_arm,
    missing = "no",
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} ({p}%)"
    )
  ) %>%
  modify_header(label = "") %>%
  modify_caption("**Table X: Baseline Characteristics by IPTp Treatment Arm**")

# Print Table X
baseline_table_treatment

```

*Table 2: Outcome Measures and Malaria Exposure Variables Stratified by IPTp Regimen*

```{r}
# Load necessary libraries
library(dplyr)
library(gtsummary)
library(here)

# Read in the cleaned data (if not already read)
promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))

# Recode variables as needed for outcomes analysis:
promo_data_clean <- promo_data_clean %>%
  mutate(
    # Create a composite adverse outcome variable: 1 if any of preterm birth, stillbirth, or low birth weight (<2.5 kg) occurs
    low_birth_weight = ifelse(birth_weight < 2.5, 1, 0),
    adverse_birth_outcome = ifelse(preterm_births_count > 0 | stillbirth_bin == 1 | low_birth_weight == 1, 1, 0),
    
    # Recode Preterm Births Count as before (0/1 -> "1", 2 -> "2")
    PretermBirths_cat = case_when(
      preterm_births_count %in% c(0, 1) ~ "1",
      preterm_births_count == 2 ~ "2"
    ),
    PretermBirths_cat = factor(PretermBirths_cat, levels = c("1", "2"))
  )

# Create Table Z: Outcome Variables by IPTp Treatment Arm with p-values
table_outcomes <- promo_data_clean %>%
  dplyr::select(
    study_arm,
    `Malaria Infection Rate During Pregnancy` = malaria_infection_rate_during_pregnancy,
    `Placental Malaria (Rogerson Criteria)` = placental_malaria_by_rogerson_criteria,
    `Preterm Births Count` = PretermBirths_cat,
    `Stillbirth bin` = stillbirth_bin,
    `Birthweight` = birth_weight,
    `Composite Adverse Outcome` = adverse_birth_outcome
  ) %>%
  tbl_summary(
    by = study_arm,
    missing = "no",
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} ({p}%)"
    )
  ) %>%
  add_p() %>%
  modify_header(label = "") %>%
  modify_caption("")

# Print Table Z
table_outcomes

```

# Visualization

*Figure 1: Histogram of Total Malaria Episodes by IPTp Treatment Arm*

```{r}
library(ggplot2)

ggplot(promo_data_clean, aes(x = total_malaria_episodes, fill = study_arm)) +
  geom_histogram(binwidth = 1, alpha = 0.6, position = "dodge") +
  labs(title = "Distribution of Total Malaria Episodes by IPTp Treatment Arm",
       x = "Total Malaria Episodes",
       y = "Frequency",
       fill = "Treatment Arm") +
  theme_minimal()

```

*Bar Graph of Total Malaria Episodes During Pregnancy by Treatment Arm*

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(here)

# Read in the cleaned data
promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))

# Recode 'total_malaria_episodes_during_pregnancy' into a categorical variable
promo_data_clean <- promo_data_clean %>%
  mutate(
    MalariaEpisodesPreg_cat = case_when(
      total_malaria_episodes_during_pregnancy %in% c(0, 1) ~ "1",
      total_malaria_episodes_during_pregnancy %in% c(2, 3) ~ "2–3",
      total_malaria_episodes_during_pregnancy >= 4 ~ "≥4"
    ),
    MalariaEpisodesPreg_cat = factor(MalariaEpisodesPreg_cat, levels = c("1", "2–3", "≥4"))
  )

# Create the bar graph
ggplot(promo_data_clean, aes(x = MalariaEpisodesPreg_cat, fill = study_arm)) +
  geom_bar(position = "dodge") +
  labs(title = "Total Malaria Episodes During Pregnancy by Treatment Arm",
       x = "Total Malaria Episodes During Pregnancy (Categorical)",
       y = "Count",
       fill = "Treatment Arm") +
  theme_minimal()


```






*Bar Graphs for Gravidity and Parity*

```{r}
# Load necessary libraries

# Read in the cleaned data
promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))

# Recode Gravidity into categories: "1", "2–3", "≥4"
promo_data_clean <- promo_data_clean %>%
  mutate(
    Gravidity_cat = case_when(
      gravidity == 1 ~ "1",
      gravidity %in% c(2, 3) ~ "2–3",
      gravidity >= 4 ~ "≥4"
    ),
    Gravidity_cat = factor(Gravidity_cat, levels = c("1", "2–3", "≥4"))
  )

# Create the bar graph for Gravidity by Treatment Arm
ggplot(promo_data_clean, aes(x = Gravidity_cat, fill = study_arm)) +
  geom_bar(position = "dodge") +
  labs(title = "Gravidity Distribution by Treatment Arm",
       x = "Gravidity Category (1, 2–3, ≥4)",
       y = "Count",
       fill = "Treatment Arm") +
  theme_minimal()


```

*Parity Distribution by Treatment Arm*

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(here)

# Read in the cleaned data
promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))

# Recode Parity into categories: "0", "1–2", "≥3"
promo_data_clean <- promo_data_clean %>%
  mutate(
    Parity_cat = case_when(
      parity == 0 ~ "0",
      parity %in% c(1, 2) ~ "1–2",
      parity >= 3 ~ "≥3"
    ),
    Parity_cat = factor(Parity_cat, levels = c("0", "1–2", "≥3"))
  )

# Create the bar graph for Parity by Treatment Arm
ggplot(promo_data_clean, aes(x = Parity_cat, fill = study_arm)) +
  geom_bar(position = "dodge") +
  labs(title = "Parity Distribution by Treatment Arm",
       x = "Parity Category (0, 1–2, ≥3)",
       y = "Count",
       fill = "Treatment Arm") +
  theme_minimal()


```

*Figure 1: Differential Impact of IPTp Treatment on the Relationship Between Malaria Episodes and Adverse Birth Outcomes*

```{r}
# Create the composite adverse outcome variable
promo_data_clean <- promo_data_clean %>%
  mutate(
    low_birth_weight = ifelse(birth_weight < 2.5, 1, 0),
    adverse_birth_outcome = ifelse(preterm_births_count > 0 | stillbirth_bin == 1 | low_birth_weight == 1, 1, 0)
  )

# Fit the interaction model (ensuring that adverse_birth_outcome now exists)
model_interaction <- glm(adverse_birth_outcome ~ total_malaria_episodes * study_arm + 
                           age_at_enrollment_years + gravidity + education_level,
                         family = binomial(link = "logit"),
                         data = promo_data_clean)

# Generate predicted probabilities over the range of total malaria episodes by study arm
pred <- ggeffect(model_interaction, terms = c("total_malaria_episodes [all]", "study_arm"))

# Plot the predicted probabilities
interaction_plot <- ggplot(pred, aes(x = x, y = predicted, color = group)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = group), alpha = 0.2, color = NA) +
  labs(title = "Differential Impact of IPTp Treatment on the Relationship\nBetween Malaria Episodes and Adverse Birth Outcomes",
       x = "Total Malaria Episodes",
       y = "Predicted Probability of Adverse Outcome",
       color = "Treatment Arm",
       fill = "Treatment Arm") +
  theme_minimal()

interaction_plot

```

*Research questions 1& 2*

```{r}
# Convert date columns to Date objects
promo_data_clean <- promo_data_clean %>%
  mutate(
    enrollment_date = as.Date(enrollment_date, format = "%Y-%m-%d"),
    withdrawal_date = as.Date(withdrawal_date, format = "%Y-%m-%d"),
    child_withdrawal_date = as.Date(child_withdrawal_date, format = "%Y-%m-%d")
  )

# Convert key categorical variables to factors
promo_data_clean <- promo_data_clean %>%
  mutate(
    study_arm = as.factor(study_arm),
    fathers_consent_for_unborn_child = as.factor(fathers_consent_for_unborn_child),
    education_level = as.factor(education_level),
    alcohol_use = as.factor(alcohol_use),
    tobacco_use = as.factor(tobacco_use),
    drug_use = as.factor(drug_use),
    hypertension = as.factor(hypertension),
    diabetes_mellitus = as.factor(diabetes_mellitus),
    rheumatic_fever = as.factor(rheumatic_fever),
    cardiac_disease = as.factor(cardiac_disease),
    renal_disease = as.factor(renal_disease),
    asthma = as.factor(asthma),
    sickle_cell_disease = as.factor(sickle_cell_disease),
    placental_malaria = as.factor(placental_malaria),
    preeclampsia = as.factor(preeclampsia),
    dp_treatment = as.factor(dp_treatment)
  )

# Check for missing values in each column
missing_values <- sapply(promo_data_clean, function(x) sum(is.na(x)))
print(missing_values)



# Ensure the 'data/clean' directory exists
if (!dir.exists(here("data", "clean"))) {
  dir.create(here("data", "clean"), recursive = TRUE)
}

# Save the cleaned data
write.csv(promo_data_clean, here("data", "clean", "PROMO_Data_clean.csv"), row.names = FALSE)


# Read in the cleaned data
promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))


```














################################################################################################################################################################## 

*Question 1: "Does the type of IPTp regimen modify the association between malaria episode frequency and adverse birth outcomes in Ugandan pregnant women?"*

```{r}
# Read in the cleaned data
promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))

# Create the composite adverse outcome variable:
# adverse_birth_outcome = 1 if any of the following occur:
# preterm birth (preterm_births_count > 0), stillbirth (stillbirth_bin == 1), or low birth weight (<2.5 kg)
promo_data_clean <- promo_data_clean %>%
  mutate(
    low_birth_weight = ifelse(birth_weight < 2.5, 1, 0),
    adverse_birth_outcome = ifelse(preterm_births_count > 0 | stillbirth_bin == 1 | low_birth_weight == 1, 1, 0)
  )

# Fit the logistic regression model with an interaction term
model_interaction <- glm(adverse_birth_outcome ~ total_malaria_episodes * study_arm +
                           age_at_enrollment_years + gravidity + education_level,
                         family = binomial(link = "logit"),
                         data = promo_data_clean)

# Display the model summary
summary(model_interaction)

# Tidy the model output (exponentiating coefficients to yield odds ratios)
tidy_model <- tidy(model_interaction, exponentiate = TRUE, conf.int = TRUE)
print(tidy_model)

# Generate predicted probabilities over the range of total malaria episodes by treatment arm
pred <- ggeffect(model_interaction, terms = c("total_malaria_episodes [all]", "study_arm"))

# Create the interaction plot
interaction_plot <- ggplot(pred, aes(x = x, y = predicted, color = group)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = group), alpha = 0.2, color = NA) +
  labs(title = "Differential Impact of IPTp Treatment on the Relationship\nBetween Malaria Episodes and Adverse Birth Outcomes",
       x = "Total Malaria Episodes",
       y = "Predicted Probability of Adverse Outcome",
       color = "Treatment Arm",
       fill = "Treatment Arm") +
  theme_minimal()

interaction_plot

```

*Table 3: Interaction Between Malaria Exposure and IPTp Treatment Arm in Predicting Adverse Birth Outcomes*

```{r}
# Tidy the model output with exponentiated coefficients (odds ratios)
tidy_model <- tidy(model_interaction, exponentiate = TRUE, conf.int = TRUE)

# Remove the intercept row and recode variable names using case_when()
tidy_model_filtered <- tidy_model %>%
  filter(term != "(Intercept)") %>%
  mutate(
    term = case_when(
      term == "total_malaria_episodes" ~ "Total Malaria Episodes",
      term == "study_armSP" ~ "Treatment Arm (SP)",
      term == "age_at_enrollment_years" ~ "Age at Enrollment (years)",
      term == "gravidity" ~ "Gravidity",
      term == "education_levelSecondary" ~ "Secondary Education",
      term == "education_levelTertiary" ~ "Tertiary Education",
      term == "education_levelUniversity" ~ "University Education",
      term == "total_malaria_episodes:study_armSP" ~ "Interaction: Malaria Episodes × SP",
      TRUE ~ term
    )
  )

# Round key values for clarity
tidy_model_filtered <- tidy_model_filtered %>%
  mutate(
    estimate = round(estimate, 2),
    std.error = round(std.error, 2),
    statistic = round(statistic, 2),
    p.value = round(p.value, 3),
    conf.low = round(conf.low, 2),
    conf.high = round(conf.high, 2)
  )

# Render the table with a blank header for the first column
kable(
  tidy_model_filtered,
  format = "pandoc",
  caption = "*Table X: Regression Results with VIFs*",
  col.names = c("", "Odds Ratio", "Std. Error", "z value", "p-value", "95% CI Lower", "95% CI Upper")
) %>% 
  kable_styling(full_width = FALSE, position = "center")

```

*Fit a logistic regression model without the interaction term*

```{r}
# Fit a logistic regression model without the interaction term
model_no_interaction <- glm(adverse_birth_outcome ~ total_malaria_episodes + study_arm +
                              age_at_enrollment_years + gravidity + education_level,
                            family = binomial(link = "logit"),
                            data = promo_data_clean)

# Perform a Likelihood Ratio Test comparing models with and without the interaction
lrt <- anova(model_no_interaction, model_interaction, test = "LRT")
print(lrt)

# Check for multicollinearity using the VIF (Variance Inflation Factor)
library(car)
vif_values <- vif(model_interaction)
print(vif_values)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(knitr)
library(kableExtra)
library(here)

# Generate Analysis of Deviance Table (Likelihood Ratio Test)
anova(model_no_interaction, model_interaction, test = "LRT")
anova_df <- as.data.frame(anova_out)
anova_df$Model <- c("Model 1 (No Interaction)", "Model 2 (With Interaction)")
anova_df <- anova_df %>% 
  select(Model, `Resid. Df`, `Resid. Dev`, Df, Deviance, `Pr(>Chi)`)
colnames(anova_df) <- c("Model", "Residual Df", "Residual Dev", "Df", "Deviance", "p-value")

kable(
  anova_df,
  format = "pandoc",
  caption = "*Table X: Analysis of Deviance Comparing Models With and Without Interaction*"
) %>% 
  kable_styling(full_width = FALSE, position = "center")

```

```{r}
library(dplyr)
library(knitr)
library(kableExtra)
library(car)
library(here)

# Compute Variance Inflation Factors (VIF) for Model 2 (with interaction)
vif_vals <- vif(model2)
vif_df <- as.data.frame(vif_vals)
vif_df$Variable <- rownames(vif_vals)
vif_df <- vif_df %>% rename(`Df (VIF)` = Df)
vif_df <- vif_df %>% 
  select(Variable, GVIF, `Df (VIF)`, `GVIF^(1/(2*Df))`) %>%
  mutate(
    Variable = case_when(
      Variable == "total_malaria_episodes" ~ "Total Malaria Episodes",
      Variable == "study_arm" ~ "Treatment Arm",
      Variable == "age_at_enrollment_years" ~ "Age at Enrollment",
      Variable == "gravidity" ~ "Gravidity",
      Variable == "education_level" ~ "Education Level",
      Variable == "total_malaria_episodes:study_arm" ~ "Interaction: Malaria Episodes × SP",
      TRUE ~ Variable
    )
  )

kable(
  vif_df,
  format = "pandoc",
  caption = "*Table Y: Variance Inflation Factors for Model 2 (With Interaction)*"
) %>% 
  kable_styling(full_width = FALSE, position = "center")

```

################################################################################################################################################################## 














## Research Question 2:

*"Among younger (women under 25 years old) Ugandan pregnant women, is increased gravidity associated with a reduced risk of adverse birth outcomes?"*

```{r}
# Subset the data to include only women under 25 years of age
promo_data_young <- promo_data_clean %>%
  filter(age_at_enrollment_years < 25)

# Create the composite adverse birth outcome variable if not already done
promo_data_young <- promo_data_young %>%
  mutate(
    low_birth_weight = ifelse(birth_weight < 2.5, 1, 0),
    adverse_birth_outcome = ifelse(preterm_births_count > 0 | stillbirth_bin == 1 | low_birth_weight == 1, 1, 0)
  )

# Fit the logistic regression model
model_gravidity <- glm(adverse_birth_outcome ~ gravidity + total_malaria_episodes + study_arm + education_level,
                       family = binomial(link = "logit"),
                       data = promo_data_young)

# Display the model summary
summary(model_gravidity)

# Tidy the model output, exponentiating coefficients to yield odds ratios
tidy_model_gravidity <- tidy(model_gravidity, exponentiate = TRUE, conf.int = TRUE)
print(tidy_model_gravidity)

# Optionally, plot predicted probabilities for gravidity
library(ggeffects)
predicted_probs <- ggeffect(model_gravidity, terms = "gravidity")
ggplot(predicted_probs, aes(x = x, y = predicted)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2, fill = "blue") +
  labs(title = "Predicted Probability of Adverse Outcome by Gravidity (Age < 25)",
       x = "Gravidity",
       y = "Predicted Probability") +
  theme_minimal()

```



*Table 6: Adjusted Odds Ratios for Adverse Birth Outcomes Among Young Pregnant Women (<25 Years)*

```{r}
# Load required libraries
library(broom)
library(dplyr)
library(knitr)
library(kableExtra)

# Assume model_gravidity is your fitted logistic regression model for women < 25
# For example:
# model_gravidity <- glm(adverse_birth_outcome ~ gravidity + total_malaria_episodes + 
#                          study_arm + education_level,
#                        family = binomial(link = "logit"),
#                        data = promo_data_young)

# Tidy the model output, exponentiating coefficients to get odds ratios
tidy_model_gravidity <- tidy(model_gravidity, exponentiate = TRUE, conf.int = TRUE)

# Create a summary table without the intercept and with neat variable names
result_table <- tidy_model_gravidity %>%
  filter(term != "(Intercept)") %>%
  mutate(
    term = case_when(
      term == "gravidity" ~ "Gravidity",
      term == "total_malaria_episodes" ~ "Total Malaria Episodes",
      term == "study_armSP" ~ "Treatment Arm (SP)",
      term == "education_levelSecondary" ~ "Secondary Education",
      term == "education_levelTertiary" ~ "Tertiary Education",
      term == "education_levelUniversity" ~ "University Education",
      TRUE ~ term
    ),
    `Odds Ratio (95% CI)` = paste0(round(estimate, 2),
                                   " (", round(conf.low, 2), ", ", round(conf.high, 2), ")")
  ) %>%
  select(term, `Odds Ratio (95% CI)`, p.value) %>%
  rename(Variable = term, `p-value` = p.value)

# Print the table using kable
result_table %>%
  kable(format = "markdown", caption = "")

```





















*Logistic Regression Model Fitting and Summary*

```{r}
# Subset the data to include only women under 25 years of age and create the composite adverse outcome variable
promo_data_young <- promo_data_clean %>%
  filter(age_at_enrollment_years < 25) %>%
  mutate(
    low_birth_weight = ifelse(birth_weight < 2.5, 1, 0),
    adverse_birth_outcome = ifelse(preterm_births_count > 0 | stillbirth_bin == 1 | low_birth_weight == 1, 1, 0)
  )


# Fit the logistic regression model for Research Question 2
model_gravidity <- glm(adverse_birth_outcome ~ gravidity + total_malaria_episodes + study_arm + education_level,
                       family = binomial(link = "logit"),
                       data = promo_data_young)

# Display the model summary
summary(model_gravidity)

# Tidy the model output with exponentiated coefficients (odds ratios)
tidy_model_gravidity <- tidy(model_gravidity, exponentiate = TRUE, conf.int = TRUE)
print(tidy_model_gravidity)


```





```{r}
# Tidy the model output and exponentiate coefficients to get odds ratios
tidy_model_gravidity <- tidy(model_gravidity, exponentiate = TRUE, conf.int = TRUE)

# Create a summary table, remove the intercept, and recode variable names
result_table <- tidy_model_gravidity %>%
  filter(term != "(Intercept)") %>%
  mutate(
    term = dplyr::recode(term,
                         "gravidity" = "Gravidity",
                         "total_malaria_episodes" = "Total Malaria Episodes",
                         "study_armSP" = "Treatment Arm (SP)",
                         "education_levelSecondary" = "Secondary Education",
                         "education_levelTertiary" = "Tertiary Education",
                         "education_levelUniversity" = "University Education"),
    `Odds Ratio (95% CI)` = paste0(round(estimate, 2),
                                   " (", round(conf.low, 2), ", ", round(conf.high, 2), ")")
  ) %>%
  select(term, `Odds Ratio (95% CI)`, p.value) %>%
  rename(Variable = term, `p-value` = p.value)

# Print the table using kable with a descriptive caption
result_table %>%
  kable(format = "markdown", 
        caption = "Adjusted Odds Ratios for Adverse Birth Outcomes Among Young Pregnant Women (<25 Years)")

```


















*Predicted Probability of Adverse Outcome by Gravidity (Age < 25)*

```{r}
# Load necessary package for generating predicted effects
library(ggeffects)

# Generate predicted probabilities over the range of gravidity
predicted_probs <- ggeffect(model_gravidity, terms = "gravidity")

# Create a scatter/line plot of predicted probabilities by gravidity
ggplot(predicted_probs, aes(x = x, y = predicted)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2, fill = "blue") +
  labs(title = "",
       x = "Gravidity",
       y = "Predicted Probability") +
  theme_minimal()

```

In my logistic regression model, I found that gravidity has a statistically significant protective effect (OR = 0.857, 95% CI: 0.737–0.995, p = 0.044), suggesting that each additional pregnancy among young women under 25 reduces the odds of adverse birth outcomes. In contrast, total malaria episodes, treatment arm, and education level were not statistically significant predictors. This indicates that, within this subgroup, prior pregnancy experience is the key factor influencing birth outcomes, while other factors seem to have little effect.

In my model, gravidity has an odds ratio of 0.857 (95% CI: 0.737–0.995, p = 0.044), indicating that each additional pregnancy among women under 25 reduces the odds of adverse birth outcomes by roughly 14%. Other predictors (total malaria episodes, treatment arm, and education level) were not statistically significant, suggesting that gravidity is the key protective factor in this subgroup.










################################################################################################################################################################## 










```{r}
# Load necessary package for ROC analysis
library(pROC)

# Generate predicted probabilities using the logistic regression model (model_gravidity)
promo_data_young$predicted_prob <- predict(model_gravidity, type = "response")

# Create the ROC curve
roc_obj <- roc(promo_data_young$adverse_birth_outcome, promo_data_young$predicted_prob)
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve for Model: Gravidity in Women < 25")
auc_value <- auc(roc_obj)
print(paste("AUC:", round(auc_value, 2)))

# Load necessary package for calibration plot
library(caret)

# Create a calibration plot
calibration_data <- data.frame(
  observed = factor(promo_data_young$adverse_birth_outcome, levels = c(0,1)),
  predicted = promo_data_young$predicted_prob
)

# Use the calibration function from the caret package
cal_plot <- calibration(observed ~ predicted, data = calibration_data, class = "1")
plot(cal_plot, main = "Calibration Plot for Model: Gravidity in Women < 25")

```

I see that the ROC curve for my model is close to the diagonal, with an AUC only slightly above 0.5. This tells me that the model doesn't have strong discriminative ability for predicting adverse outcomes in women under 25. Additionally, the calibration plot shows that my predicted probabilities often stray from the ideal diagonal—especially in the mid-range—indicating that my model’s risk estimates don't consistently match the observed rates. Overall, while gravidity is statistically significant, my model as a whole isn't very effective at distinguishing between those who experience adverse outcomes and those who don't, and its probability estimates need improvement.




*Train/Test Split and Logistic Regression (Interaction Model)* *Research Question 1*

ML (Research Question 1. I built and evaluated logistic regression, random forest, and boosting models (using cross-validation and train/test splits) to assess how IPTp regimen modifies the impact of malaria episodes on adverse birth outcomes.)

Research Question 1, which examines whether the IPTp regimen modifies the association between malaria episodes and adverse birth outcomes.

Below is an example of how I can extend my analysis by performing a train/test split and comparing a couple of machine learning classification models. For Research Question 1 (the logistic regression with the interaction term), I'll split the data into training (70%) and test (30%) sets and then fit both a logistic regression and a random forest classifier. This lets me assess model performance on unseen data using metrics like ROC AUC.

```{r}
# Set seed and perform a 70/30 train/test split (stratified on adverse outcomes)
# Load necessary package for data splitting
library(rsample)

# Set seed and perform a 70/30 train/test split (stratified on adverse outcomes)
set.seed(1234)
split_data <- initial_split(promo_data_clean, prop = 0.7, strata = adverse_birth_outcome)
train_data <- training(split_data)
test_data  <- testing(split_data)


# Fit the logistic regression model with an interaction term on training data
model_interaction <- glm(adverse_birth_outcome ~ total_malaria_episodes * study_arm +
                           age_at_enrollment_years + gravidity + education_level,
                         family = binomial(link = "logit"),
                         data = train_data)

# Summarize the model
summary(model_interaction)

# Generate predictions on the test set and compute the ROC curve
library(pROC)
pred_test_logit <- predict(model_interaction, newdata = test_data, type = "response")
roc_logit <- roc(test_data$adverse_birth_outcome, pred_test_logit)
plot(roc_logit, col = "blue", lwd = 2, main = "ROC Curve: Logistic Regression")
auc_logit <- auc(roc_logit)
print(paste("Logistic Regression AUC:", round(auc_logit, 2)))

```

*Train/Test Split and Random Forest Model* which fits a random forest model and plots its ROC curve, is also used for Research Question 1 as an alternative ML approach

```{r}
# Ensure the outcome is a factor in train_data
train_data <- train_data %>%
  mutate(adverse_birth_outcome = factor(adverse_birth_outcome))

# Build a recipe for the random forest model (no inline interaction here)
rf_recipe <- recipe(adverse_birth_outcome ~ total_malaria_episodes + study_arm +
                      age_at_enrollment_years + gravidity + education_level,
                    data = train_data) %>%
  step_dummy(all_nominal_predictors())

# Create a workflow for the random forest model
rf_workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

# Fit the random forest model on training data
rf_fit <- rf_workflow %>% fit(data = train_data)

# Generate predicted probabilities on the test set
rf_preds <- predict(rf_fit, new_data = test_data, type = "prob")

# Compute ROC for the random forest using the probability for class "1"
library(pROC)
roc_rf <- roc(test_data$adverse_birth_outcome, rf_preds$.pred_1)
plot(roc_rf, col = "red", lwd = 2, main = "ROC Curve: Random Forest")
auc_rf <- auc(roc_rf)
print(paste("Random Forest AUC:", round(auc_rf, 2)))

```

-   Model Comparison and Interpretation\*

```{r}
# Compare ROC curves
plot(roc_logit, col = "blue", lwd = 2, main = "ROC Comparison: Logistic Regression vs. Random Forest")
lines(roc_rf, col = "red", lwd = 2)
legend("bottomright", legend = c("Logistic Regression", "Random Forest"),
       col = c("blue", "red"), lwd = 2)

# Optionally, you could also compare other metrics like accuracy or calibration on the test set.

```

I started by performing a 70/30 train/test split, stratified on the adverse birth outcome. For Research Question 1, I fit a logistic regression model with an interaction term on the training set, generated predictions on the test set, and plotted the ROC curve (with an AUC computed to assess discriminative performance). Then, I fit a random forest classifier (converting factor predictors to dummies in the recipe) and similarly evaluated its performance on the test set using ROC analysis. Finally, I overlaid both ROC curves to compare the models directly. This approach provides an "honest" assessment of model performance on unseen data and allows me to explore additional machine learning methods for addressing my research question.

From these plots, I see that both the logistic regression (blue) and random forest (red) models produce ROC curves only slightly above the diagonal, indicating that neither model has strong discriminative ability for predicting adverse outcomes in this dataset. In the combined plot, the logistic regression curve sits just above the random forest curve, suggesting it might be marginally better, but the difference is small—both are near an AUC of around 0.55–0.60. Overall, these results tell me that, with my current predictors and data, neither model reliably distinguishes between those who experience adverse birth outcomes and those who do not.

```{r}
# Load necessary packages (if not already loaded)
library(tidymodels)
library(xgboost)

# Define a boosting model for classification (using boost_tree)
boost_model <- boost_tree(
  mode = "classification",
  trees = 500,
  learn_rate = tune(),
  tree_depth = tune()
) %>%
  set_engine("xgboost")

# Build a recipe (convert factor predictors to dummy variables)
boost_recipe <- recipe(adverse_birth_outcome ~ total_malaria_episodes + study_arm +
                         age_at_enrollment_years + gravidity + education_level,
                       data = train_data) %>%
  step_dummy(all_nominal_predictors())

# Create a workflow
boost_workflow <- workflow() %>%
  add_model(boost_model) %>%
  add_recipe(boost_recipe)

# Create a tuning grid for learning rate and tree depth
boost_grid <- grid_regular(
  learn_rate(range = c(0.01, 0.3)),
  tree_depth(range = c(3, 8)),
  levels = 5
)

# Create 10-fold cross-validation folds from the training data
cv_folds <- vfold_cv(train_data, v = 10)

# Perform 10-fold cross-validation tuning using the boosting model workflow
boost_tune <- tune_grid(
  boost_workflow,
  resamples = cv_folds,
  grid = boost_grid
)


# Select best parameters based on ROC AUC
best_boost <- select_best(boost_tune, metric = "roc_auc")

# Finalize the workflow with best parameters
final_boost <- finalize_workflow(boost_workflow, best_boost)

# Fit the final boosting model on the training data
boost_fit <- final_boost %>% fit(data = train_data)

# Generate predicted probabilities on the test set
boost_preds <- predict(boost_fit, new_data = test_data, type = "prob")

# Compute ROC for the boosting model using the probability for class "1"
roc_boost <- roc(test_data$adverse_birth_outcome, boost_preds$.pred_1)
plot(roc_boost, col = "green", lwd = 2, main = "ROC Curve: Boosting Model")
auc_boost <- auc(roc_boost)
print(paste("Boosting Model AUC:", round(auc_boost, 2)))

```

The ROC curve for the boosting model rises more clearly above the diagonal line than the previous models, suggesting it does a better job distinguishing between those who experience adverse birth outcomes and those who do not. Although it’s still not perfect, the curve indicates an improvement in predictive performance compared to the logistic regression and random forest models, meaning the boosting model likely has a higher AUC and provides a more accurate risk estimate for adverse outcomes in this dataset.

*ML for research question 2*

*Subset Data and Create Outcome Variable & Fit Logistic Regression Model*

```{r}
# Subset data to include only women under 25 years
promo_data_young <- promo_data_clean %>%
  filter(age_at_enrollment_years < 25)

# Create the composite adverse outcome variable:
# adverse_birth_outcome = 1 if any of the following occur:
# preterm birth (preterm_births_count > 0), stillbirth (stillbirth_bin == 1), or low birth weight (<2.5 kg)
promo_data_young <- promo_data_young %>%
  mutate(
    low_birth_weight = ifelse(birth_weight < 2.5, 1, 0),
    adverse_birth_outcome = ifelse(preterm_births_count > 0 | stillbirth_bin == 1 | low_birth_weight == 1, 1, 0)
  )


# Fit the logistic regression model for women under 25
model_gravidity <- glm(adverse_birth_outcome ~ gravidity + total_malaria_episodes + study_arm + education_level,
                       family = binomial(link = "logit"),
                       data = promo_data_young)

# Display the model summary and tidy output with exponentiated coefficients (odds ratios)
summary(model_gravidity)
tidy_model_gravidity <- tidy(model_gravidity, exponentiate = TRUE, conf.int = TRUE)
print(tidy_model_gravidity)


```

-   Plot Predicted Probabilities by Gravidity\*

```{r}
# Load necessary package for generating predicted effects
library(ggeffects)

# Generate predicted probabilities over the range of gravidity
predicted_probs <- ggeffect(model_gravidity, terms = "gravidity")

# Create a line plot of predicted probabilities with confidence intervals
ggplot(predicted_probs, aes(x = x, y = predicted)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2, fill = "blue") +
  labs(title = "Predicted Probability of Adverse Outcome by Gravidity (Age < 25)",
       x = "Gravidity", y = "Predicted Probability") +
  theme_minimal()

```

In this subgroup of women under 25, gravidity is the only statistically significant predictor: each additional pregnancy lowers the odds of an adverse birth outcome (OR ≈ 0.86, 95% CI: 0.74–0.995, p = 0.044). As shown in the predicted probability plot, the likelihood of an adverse outcome steadily drops from roughly 60% at gravidity = 1 to about 45% at gravidity = 5. The other predictors—total malaria episodes, treatment arm, and education level—do not significantly change the risk in this subgroup, suggesting that prior pregnancy experience is the key factor driving improved outcomes for younger women.




################################################################################################################################################################## 



## Machine Learning approaches

To complement regression analysis, we implemented machine learning workflows for Research Question 1. All models were trained on 70% of the data and evaluated on a held-out 30% test set. Ten-fold cross-validation on the training set was used for hyperparameter tuning where applicable.


# Prepare outcome as factor and split data

```{r}
promo_data_clean <- promo_data_clean %>%
  mutate(adverse_birth_outcome = factor(adverse_birth_outcome, levels = c(0,1)))
set.seed(2025)
split       <- initial_split(promo_data_clean, prop = 0.7, strata = adverse_birth_outcome)
train_data  <- training(split)
test_data   <- testing(split)
cv_folds    <- vfold_cv(train_data, v = 10, strata = adverse_birth_outcome)
```


### Logistic regression (baseline)

```{r}
logistic_wf <- workflow() %>%
  add_model(logistic_reg() %>% set_engine("glm")) %>%
  add_formula(adverse_birth_outcome ~ total_malaria_episodes * study_arm +
                age_at_enrollment_years + gravidity + education_level)

logistic_fit  <- fit(logistic_wf, data = train_data)

# Predict and evaluate
logistic_preds <- predict(logistic_fit, test_data, type = "prob") %>%
  bind_cols(test_data)
logistic_auc <- roc_auc(logistic_preds, truth = adverse_birth_outcome, .pred_1)
print(logistic_auc)
```
The logistic regression model achieved an AUC of 0.48 on the independent test set, indicating discrimination no better than random chance. This poor performance suggests that, in its current form, the linear predictor does not capture sufficient signal to distinguish between pregnancies with and without adverse outcomes. These findings highlight the need to revisit variable coding, explore non‑linear feature transformations, or consider more flexible algorithms (e.g., random forest or gradient boosting) to improve predictive accuracy.









### Random Forest classifier


```{r}
# Tuning grid (requires library(dials) or library(tidymodels) loaded)
rf_recipe <- recipe(adverse_birth_outcome ~ total_malaria_episodes + study_arm +
                      age_at_enrollment_years + gravidity + education_level,
                    data = train_data) %>%
  step_dummy(all_nominal_predictors())

rf_wf <- workflow() %>%
  add_model(rand_forest(trees = 500, mtry = tune(), min_n = tune()) %>%
              set_engine("ranger") %>% set_mode("classification")) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  mtry(range = c(1, 5)),
  min_n(range = c(2, 10)),
  levels = 5
)

rf_tune <- tune_grid(
  rf_wf,
  resamples = cv_folds,
  grid      = rf_grid,
  metrics   = metric_set(roc_auc)
)

best_rf <- select_best(rf_tune, metric = "roc_auc")
rf_final <- finalize_workflow(rf_wf, best_rf)
rf_fit   <- fit(rf_final, data = train_data)

rf_preds <- predict(rf_fit, test_data, type = "prob") %>%
  bind_cols(test_data)
rf_auc <- roc_auc(rf_preds, truth = adverse_birth_outcome, .pred_1)
print(rf_auc)

```
The random forest classifier achieved an AUC of 0.42 on the held‑out test set, indicating discrimination well below the 0.50 threshold expected for random performance. This result demonstrates that, despite its capacity to capture non‑linear relationships, the current set of predictors does not provide sufficient signal to distinguish between pregnancies with and without adverse outcomes. These findings suggest the need for deeper feature engineering, additional or more granular clinical variables, and exploration of alternative modeling strategies to enhance predictive performance.

















### Gradient boosting (XGBoost)

```{r}
# Define workflow
# Ensure grid_regular() is available
library(dials)

# Define workflow
xgb_wf <- workflow() %>%
  add_model(
    boost_tree(
      trees = 500,
      learn_rate = tune(),
      tree_depth = tune()
    ) %>%
      set_engine("xgboost") %>%
      set_mode("classification")
  ) %>%
  add_recipe(rf_recipe)

# Create tuning grid
xgb_grid <- grid_regular(
  learn_rate(range = c(0.01, 0.3)),
  tree_depth(range = c(3, 8)),
  levels = 4
)

# Perform tuning
xgb_tune <- tune_grid(
  xgb_wf,
  resamples = cv_folds,
  grid      = xgb_grid,
  metrics   = metric_set(roc_auc)
)

# Select best hyperparameters by naming the metric argument
best_xgb <- select_best(xgb_tune, metric = "roc_auc")

# Finalize and fit the final model
xgb_final <- finalize_workflow(xgb_wf, best_xgb)
xgb_fit   <- fit(xgb_final, data = train_data)

# Predict on the test set
xgb_preds <- predict(xgb_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

# Compute and print AUC
xgb_auc <- roc_auc(xgb_preds, truth = adverse_birth_outcome, .pred_1)
print(xgb_auc)
```
The gradient boosting model (XGBoost) achieved an AUC of 0.44 on the independent test set, indicating performance substantially below the 0.50 threshold for random guessing. Despite its flexibility in modeling complex, non‑linear relationships, the current predictor set does not appear to contain enough signal to reliably distinguish between pregnancies with and without adverse outcomes. These results highlight the need for additional feature engineering, incorporation of more informative clinical variables, or alternative analytic strategies to improve predictive accuracy.











### Model comparison on test set

```{r}
roc_lr  <- roc_curve(logistic_preds, truth = adverse_birth_outcome, .pred_1) %>% mutate(model = "Logistic")
roc_rf  <- roc_curve(rf_preds,       truth = adverse_birth_outcome, .pred_1) %>% mutate(model = "RF")
roc_xgb <- roc_curve(xgb_preds,     truth = adverse_birth_outcome, .pred_1) %>% mutate(model = "XGBoost")

roc_all <- bind_rows(roc_lr, roc_rf, roc_xgb)

ggplot(roc_all, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path(size = 1) +
  labs(title = "ROC Curves: ML Models for Adverse Outcome Prediction",
       x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()
```
Figure X displays the ROC curves for logistic regression (red), random forest (green), and gradient boosting (blue) on the held‑out test set. All three curves lie close to the diagonal, reflecting poor discrimination: none of the models reliably separates pregnancies with versus without adverse outcomes. Of the three, logistic regression achieved the highest AUC (0.48), followed by XGBoost (0.44) and random forest (0.42), but these differences are marginal and all fall below the 0.50 threshold for random performance. Collectively, these results underscore that, with the current predictor set, more informative features or alternative modeling strategies will be necessary to improve predictive accuracy.











## Subgroup ML: Young women (<25)
A similar train/test and model workflow was applied to the subset of women under 25 to assess whether ML methods capture the protective effect of gravidity in this subgroup.

```{r}
# Subset and split
set.seed(2025)
young_split <- initial_split(
  filter(promo_data_clean, age_at_enrollment_years < 25),
  prop = 0.7,
  strata = adverse_birth_outcome
)
young_train <- training(young_split) %>%
  mutate(adverse_birth_outcome = factor(adverse_birth_outcome, levels = c(0,1)))
young_test  <- testing(young_split) %>%
  mutate(adverse_birth_outcome = factor(adverse_birth_outcome, levels = c(0,1)))

# Logistic on gravidity only
young_logit <- glm(
  adverse_birth_outcome ~ gravidity,
  family = binomial(),
  data = young_train
)

# Evaluate performance
young_preds <- predict(
  young_logit,
  newdata = young_test,
  type = "response"
)
young_auc <- roc_auc_vec(
  truth = young_test$adverse_birth_outcome,
  estimate = young_preds
)
print(young_auc)
```









This ML section demonstrates train/test splitting, cross-validation for tuning, and comparison across logistic regression, random forest, and boosting models, aligned with Research Question1 and the subgroup analysis for Research Question2.

## Subgroup ML: Young women (<25)

A similar train/test and model workflow was applied to the subset of women under 25 to assess whether ML methods capture the protective effect of gravidity in this subgroup.

```{r}
# r ml_young
# Subset
young_split <- initial_split(filter(promo_data_clean, age_at_enrollment_years < 25), prop = 0.7, strata = adverse_birth_outcome)
young_train <- training(young_split)
young_test  <- testing(young_split)

# Fit logistic with gravidity only
young_logit <- glm(adverse_birth_outcome ~ gravidity,
                   family = binomial(), data = young_train)

# Evaluate on test set
young_preds <- predict(young_logit, newdata = young_test, type = "response")
roc_auc_vec <- roc_auc_vec(young_test$adverse_birth_outcome, young_preds)
print(roc_auc_vec)
```


In the subgroup of women under 25 years, the machine‑learning model achieved an AUC of 0.49 on the held‑out test set, indicating discrimination essentially equivalent to random guessing. This suggests that, even within this younger cohort, the available predictors do not provide sufficient signal to distinguish pregnancies with versus without adverse outcomes. These results highlight the need for additional or more granular features and potentially alternative modeling strategies to meaningfully improve risk stratification in this subgroup.









This ML section demonstrates train/test splitting, cross-validation for tuning, and comparison across logistic regression, random forest, and boosting models, aligned with Research Question1 and the subgroup analysis for Question2.








################################################################################################################################################################## 





### Improved ML: Elastic Net with Feature Engineering

```{r ml_improved_setup, message=FALSE, warning=FALSE}
# 0) Load all needed packages
library(tidymodels)  # recipes, parsnip, workflows, tune, yardstick, dials
library(themis)      # for step_smote()
```

```{r ml_improved_recipe, message=FALSE}
# 1) Encode factors, engineer features, balance classes, normalize
improved_recipe <- recipe(adverse_birth_outcome ~ 
                             total_malaria_episodes + study_arm +
                             age_at_enrollment_years + gravidity + education_level,
                           data = train_data) %>%
  # a) one‑hot encode all factor predictors
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  # b) create polynomial & interaction features
  step_mutate(
    malaria_sq       = total_malaria_episodes^2,
    malaria_DP_inter = total_malaria_episodes * study_arm_DP
  ) %>%
  # c) up‑sample minority class via SMOTE
  step_smote(adverse_birth_outcome) %>%
  # d) center & scale all predictors
  step_normalize(all_predictors())
                          # (5)

```




```{r}
# 2) Specify elastic‑net logistic regression
enet_spec <- logistic_reg(
    penalty = tune(),   # strength of regularization
    mixture = tune()    # 0=ridge ↔ 1=lasso
  ) %>%
  set_engine("glmnet")

```



```{r}
# 3) Build the workflow
enet_wf <- workflow() %>%
  add_recipe(improved_recipe) %>%
  add_model(enet_spec)

```





```{r}
# 4) Create a tuning grid & 5) tune via 10‑fold CV
enet_grid <- grid_regular(
  penalty(range = c(-5, -1), trans = log10_trans()),  # 1e-5 → 1e-1
  mixture(range = c(0, 1)),
  levels = 5
)

enet_tune <- tune_grid(
  enet_wf,
  resamples = cv_folds,
  grid      = enet_grid,
  metrics   = metric_set(roc_auc)
)

```

```{r}
# 6) Select best, finalize, fit on train, & evaluate on test
best_enet  <- select_best(enet_tune, metric = "roc_auc")
enet_final <- finalize_workflow(enet_wf, best_enet)
enet_fit   <- fit(enet_final, data = train_data)

enet_preds <- predict(enet_fit, test_data, type = "prob") %>% bind_cols(test_data)
enet_auc   <- roc_auc(enet_preds, truth = adverse_birth_outcome, .pred_1)
print(enet_auc)

```
The elastic‑net model achieved an AUC of 0.49 on the held‑out test set—below the 0.50 threshold for random guessing. This indicates that, even with polynomial, interaction, and SMOTE steps, the model cannot reliably distinguish between pregnancies with and without adverse outcomes. Such near‑random performance suggests either insufficient predictive signal in the available features or the need for additional, more informative variables and further model refinement.




################################################################################################################################################################## 



```{r}
# Create temporal & composite features in the training set
train_data <- train_data %>%
  mutate(
    # Month of enrollment (seasonality)
    enroll_month = lubridate::month(enrollment_date),
    # Episodes per trimester (assuming 40‑week gestation)
    episodes_per_trimester = total_malaria_episodes / (gestational_age_at_enrollment_weeks / (40/3)),
    # Socioeconomic index (if available)
    # wealth_index = as.numeric(education_level) * household_assets_score
  )

# Apply same transformations to test_data
test_data <- test_data %>%
  mutate(
    enroll_month = lubridate::month(enrollment_date),
    episodes_per_trimester = total_malaria_episodes / (gestational_age_at_enrollment_weeks / (40/3))
  )

```


```{r}
library(themis)

imbalanced_recipe <- recipe(adverse_birth_outcome ~ ., data = train_data) %>%
  # standard preprocessing
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  # adaptive synthetic sampling
  step_adasyn(adverse_birth_outcome) %>%
  step_normalize(all_predictors())

```


```{r}
# Continue using enet_wf from before
set.seed(2025)
enet_bayes <- tune_bayes(
  enet_wf,
  resamples = cv_folds,
  initial = enet_tune,       # seed the search from your grid results
  iter    = 20,              # number of Bayesian iterations
  metrics = metric_set(roc_auc)
)

best_enet_bayes <- select_best(enet_bayes, metric = "roc_auc")

```









```{r}
library(stacks)

# Assume you've tuned logistic, rf, xgb already: logistic_tune, rf_tune, xgb_tune
stack <- stacks() %>%
  add_candidates(logistic_tune) %>%
  add_candidates(rf_tune)       %>%
  add_candidates(xgb_tune)      %>%
  blend_predictions()           %>%
  fit_members()

# Evaluate on test set
stack_preds <- predict(stack, test_data, type = "prob") %>% bind_cols(test_data)
stack_auc   <- roc_auc(stack_preds, truth = adverse_birth_outcome, .pred_1)
print(stack_auc)

```






```{r}
svm_spec <- svm_rbf(
    cost      = tune(),
    rbf_sigma = tune()
  ) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

svm_wf <- workflow() %>%
  add_model(svm_spec) %>%
  add_recipe(improved_recipe)  # or your imbalanced_recipe

svm_grid <- grid_regular(
  cost(range = c(-5, 2), trans = log2_trans()),
  rbf_sigma(range = c(-3, 0), trans = log2_trans()),
  levels = 5
)

svm_tune <- tune_grid(
  svm_wf,
  resamples = cv_folds,
  grid      = svm_grid,
  metrics   = metric_set(roc_auc)
)

best_svm <- select_best(svm_tune, metric = "roc_auc")

```



```{r}
# For your best model (e.g. enet_fit or stack)
pr    <- pr_curve(stack_preds, truth = adverse_birth_outcome, .pred_1)
pr_auc <- pr_auc(stack_preds, truth = adverse_birth_outcome, .pred_1)

autoplot(pr) +
  labs(title = "Precision–Recall Curve", subtitle = paste("PR AUC =", round(pr_auc$.estimate,2)))

```





