---
title: "Supplementary Materials of Impact of IPTp Regimen on Pregnancy Outcomes"
format:
  pdf:
    toc: false
    number-sections: true
    highlight-style: github
bibliography:
  - ../../../assets/dataanalysis-references.bib
csl: ../../../assets/american-journal-of-epidemiology.csl
---




{{< pagebreak >}}




# Overview
The Supplementary Appendix begins with comprehensive methodological details, including variable‐by‐variable missingness (Table S1), analysis of deviance comparing models with and without the malaria×SP interaction (Table S2), and variance inflation factors for the final interaction model (Table S3). It also presents machine‐learning tuning results with the top five elastic-net hyperparameter combinations ranked by mean cross-validated AUC (Table S4) alongside a heatmap illustrating AUC across the penalty–mixture grid (Figure S1). Full code excerpts document our data-cleaning steps, rsample splits, recipe definitions, and tune_grid workflows. The appendix then moves on to additional results: a detailed stratification of outcome measures and malaria-exposure variables by IPTp arm (Table S5), bar graphs of total malaria episodes during pregnancy (Figure S2), gravidity (Figure S3), and parity (Figure S4) distributions by treatment arm, and a bootstrap calibration curve for the interaction model (Figure S5). Finally, it compares model discrimination with overlaid ROC curves for logistic regression, random forest, and XGBoost (Figure S6), presents a precision-recall curve for the top‐performing machine-learning model (Figure S7), and reports the test-set AUC for the gravidity-only model in women under 25 years (Table S6).


# Code and file information

Explain here what each code/file is and does, and in which order (if any) users need to run thing to reproduce everything.
Essentially, give a full set of instructions to re-generate everything.




```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.show="hold"}
library(here)
here::i_am("products/manuscript/Manuscript.qmd")

# ← THIS is the one that actually sets the working dir for *all* chunks:
library(knitr)
knitr::opts_chunk$set(root.dir = here::here("products/manuscript"))

```



```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.show="hold"}
# Load Required Libraries for Data Handling, Visualization, and Analysis

# Path management
library(here)               # File path handling

# Data manipulation and wrangling
library(dplyr)              # Data manipulation
library(tidyverse)          # Core tidyverse packages (ggplot2, readr, etc.)
library(janitor)            # Clean column names
library(skimr)              # Quick data summaries
library(lubridate)          # Date/time handling
library(forcats)            # Categorical variable tools

# Visualization
library(ggplot2)            # Data visualization
library(ggpubr)             # Publication-ready plots

# Tables and reporting
library(gtsummary)          # Summary tables
library(gt)                 # Advanced table formatting
library(knitr)              # Report generation
library(kableExtra)         # Enhanced markdown tables

# Data exploration & preparation
library(Amelia)             # Missing-data visualization
library(pwr)                # Power analysis
library(DiagrammeR)         # Diagrams and flowcharts

# Survival analysis
library(survival)           # Survival models
library(survminer)          # Survival plots

# Model effects & outputs
library(ggeffects)          # Marginal effects extraction
library(broom)              # Tidy model outputs

# Machine learning & modeling
library(tidymodels)         # Modeling framework (recipes, parsnip, workflows, tune, yardstick, rsample)
library(themis)             # Class-imbalance sampling (e.g., SMOTE)
library(dials)              # Parameter tuning grids
library(ranger)             # Random forest engine
library(xgboost)            # Gradient boosting engine
library(generalhoslem)
library(ResourceSelection)
library(rms)
```



```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.show="hold"}
# Data Import & Initial Inspection 
# load the Dataset
PROMO_Data <- read_csv(here("data", "raw-data", "PROMO_Data.csv"))


promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))
```












{{< pagebreak >}}


# Additional Method Details

Often, the main manuscript only allows for an overview description of the methods. Use the supplement to describe all your methods, models and approaches in a lot of detail. Reference specific parts of your code as needed.

{{< pagebreak >}}


# Additional results

Show additional results here. Those can be some useful exploratory/descriptive figures or tables, or results from additional analyses that didn't make it into the main text.


*Bar Graph of Total Malaria Episodes During Pregnancy by Treatment Arm*

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(here)

# Read in the cleaned data
promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))

# Recode 'total_malaria_episodes_during_pregnancy' into a categorical variable
promo_data_clean <- promo_data_clean %>%
  mutate(
    MalariaEpisodesPreg_cat = case_when(
      total_malaria_episodes_during_pregnancy %in% c(0, 1) ~ "1",
      total_malaria_episodes_during_pregnancy %in% c(2, 3) ~ "2–3",
      total_malaria_episodes_during_pregnancy >= 4 ~ "≥4"
    ),
    MalariaEpisodesPreg_cat = factor(MalariaEpisodesPreg_cat, levels = c("1", "2–3", "≥4"))
  )

# Create the bar graph
ggplot(promo_data_clean, aes(x = MalariaEpisodesPreg_cat, fill = study_arm)) +
  geom_bar(position = "dodge") +
  labs(title = "Total Malaria Episodes During Pregnancy by Treatment Arm",
       x = "Total Malaria Episodes During Pregnancy (Categorical)",
       y = "Count",
       fill = "Treatment Arm") +
  theme_minimal()


```







*Bar Graphs for Gravidity and Parity*

```{r}
# Load necessary libraries

# Read in the cleaned data
promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))

# Recode Gravidity into categories: "1", "2–3", "≥4"
promo_data_clean <- promo_data_clean %>%
  mutate(
    Gravidity_cat = case_when(
      gravidity == 1 ~ "1",
      gravidity %in% c(2, 3) ~ "2–3",
      gravidity >= 4 ~ "≥4"
    ),
    Gravidity_cat = factor(Gravidity_cat, levels = c("1", "2–3", "≥4"))
  )

# Create the bar graph for Gravidity by Treatment Arm
ggplot(promo_data_clean, aes(x = Gravidity_cat, fill = study_arm)) +
  geom_bar(position = "dodge") +
  labs(title = "Gravidity Distribution by Treatment Arm",
       x = "Gravidity Category (1, 2–3, ≥4)",
       y = "Count",
       fill = "Treatment Arm") +
  theme_minimal()


```




*Parity Distribution by Treatment Arm*

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(here)

# Read in the cleaned data
promo_data_clean <- read.csv(here("data", "clean", "PROMO_Data_clean.csv"))

# Recode Parity into categories: "0", "1–2", "≥3"
promo_data_clean <- promo_data_clean %>%
  mutate(
    Parity_cat = case_when(
      parity == 0 ~ "0",
      parity %in% c(1, 2) ~ "1–2",
      parity >= 3 ~ "≥3"
    ),
    Parity_cat = factor(Parity_cat, levels = c("0", "1–2", "≥3"))
  )

# Create the bar graph for Parity by Treatment Arm
ggplot(promo_data_clean, aes(x = Parity_cat, fill = study_arm)) +
  geom_bar(position = "dodge") +
  labs(title = "Parity Distribution by Treatment Arm",
       x = "Parity Category (0, 1–2, ≥3)",
       y = "Count",
       fill = "Treatment Arm") +
  theme_minimal()


```






## Example additional result


@tbl-resulttable1 shows an additional table summarizing a model fit.

```{r}
#| label: tbl-resulttable1
#| tbl-cap: "Another fit table."
#| echo: FALSE
resulttable1 = readRDS(here("results","tables","resulttable1.rds"))
knitr::kable(resulttable1)
```



@fig-result2 shows a scatterplot figure produced by one of the R scripts.


```{r}
#| label: fig-result2
#| fig-cap: "Height and weight."
#| echo: FALSE
knitr::include_graphics(here("results","figures","height-weight.png"))
```

#  Evaluating Discrimination (ROC Curve and AUC)


```{r roc-calibration, echo=TRUE, message=FALSE, warning=FALSE}
# 1) Load required libraries
library(dplyr)
library(pROC)
library(caret)
library(here)

# 2) Read cleaned data and subset to women < 25
promo_data_clean <- read.csv(here("data/clean/PROMO_Data_clean.csv"))
promo_data_young <- promo_data_clean %>%
  filter(age_at_enrollment_years < 25) %>%
  mutate(
    # Composite adverse outcome: 1 if preterm, stillbirth, or low birth weight
    low_birth_weight      = as.integer(birth_weight < 2.5),
    adverse_birth_outcome = as.integer(
      preterm_births_count > 0 |
      stillbirth_bin == 1        |
      low_birth_weight == 1
    )
  )

# 3) Fit logistic regression model
model_gravidity <- glm(
  adverse_birth_outcome ~ gravidity + total_malaria_episodes +
    study_arm + education_level,
  family = binomial(link = "logit"),
  data   = promo_data_young
)

# 4) Generate predicted probabilities
promo_data_young$predicted_prob <- predict(
  model_gravidity,
  newdata = promo_data_young,
  type    = "response"
)

# 5) ROC curve and AUC
roc_obj  <- roc(
  promo_data_young$adverse_birth_outcome,
  promo_data_young$predicted_prob
)
plot(
  roc_obj,
  col   = "blue",
  lwd   = 2,
  main  = "ROC Curve: Gravidity Model (Age < 25)"
)
auc_val <- auc(roc_obj)
cat("AUC:", round(auc_val, 2), "\n")

# 6) Calibration plot
data_cal <- data.frame(
  observed = factor(promo_data_young$adverse_birth_outcome, levels = c(0,1)),
  predicted = promo_data_young$predicted_prob
)
cal_plot <- calibration(
  observed ~ predicted,
  data  = data_cal,
  class = "1"
)
plot(cal_plot, main = "Calibration Plot: Gravidity Model (Age < 25)")

```

I see that the ROC curve for my model is close to the diagonal, with an AUC only slightly above 0.5. This tells me that the model doesn't have strong discriminative ability for predicting adverse outcomes in women under 25. Additionally, the calibration plot shows that my predicted probabilities often stray from the ideal diagonal—especially in the mid-range—indicating that my model’s risk estimates don't consistently match the observed rates. Overall, while gravidity is statistically significant, my model as a whole isn't very effective at distinguishing between those who experience adverse outcomes and those who don't, and its probability estimates need improvement.











```{r}
library(tidymodels)
library(readr)
library(here)

# 2. Read & split data
df <- read_csv(here("data","clean","PROMO_Data_clean.csv")) %>%
  mutate(adverse_birth_outcome = factor(adverse_birth_outcome, levels = c(0,1)))

set.seed(2025)
splits    <- initial_split(df, prop = 0.7, strata = adverse_birth_outcome)
train     <- training(splits)
test      <- testing(splits)
cv_folds  <- vfold_cv(train, v = 5, strata = adverse_birth_outcome)

# 3. Preprocessing recipe
ml_rec <- recipe(adverse_birth_outcome ~ 
                   age_at_enrollment_years +
                   gravidity +
                   total_malaria_episodes +
                   education_level +
                   study_arm +
                   normalized_malaria_rate,   # ← replaced here
                 data = train) %>%
  step_dummy(all_nominal_predictors()) %>%  # one‐hot encode
  step_zv(all_predictors()) %>%             # remove zero‐variance
  step_normalize(all_numeric_predictors())  # center & scale

# 4. Model specifications & workflows

## (a) LASSO‐penalized logistic regression
lasso_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

lasso_wf  <- workflow() %>% 
  add_model(lasso_mod) %>% 
  add_recipe(ml_rec)

## (b) Random forest
rf_mod <- rand_forest(mtry = tune(), trees = 1000, min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_wf  <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(ml_rec)

## (c) XGBoost
xgb_mod <- boost_tree(trees = tune(),
                      tree_depth = tune(),
                      learn_rate = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgb_wf <- workflow() %>% 
  add_model(xgb_mod) %>% 
  add_recipe(ml_rec)

# 5. Tuning grids & 5‐fold CV
set.seed(2025)

lasso_grid <- grid_regular(penalty(), levels = 20)

rf_params <- parameters(rf_mod) %>%
  update(mtry = mtry(c(1, 6)))
rf_grid   <- grid_latin_hypercube(rf_params, size = 20)

xgb_params <- parameters(xgb_mod)
xgb_grid   <- grid_latin_hypercube(xgb_params, size = 30)

ml_metrics <- metric_set(roc_auc)

lasso_res <- tune_grid(lasso_wf, resamples = cv_folds,
                       grid = lasso_grid, metrics = ml_metrics)

rf_res    <- tune_grid(rf_wf,    resamples = cv_folds,
                       grid = rf_grid,    metrics = ml_metrics)

xgb_res   <- tune_grid(xgb_wf,   resamples = cv_folds,
                       grid = xgb_grid,   metrics = ml_metrics)

library(dplyr)
library(knitr)

# 6) Pull the metrics into plain tibbles
lasso_metrics <- collect_metrics(lasso_res)
rf_metrics    <- collect_metrics(rf_res)
xgb_metrics   <- collect_metrics(xgb_res)

# 7) Summarize AUC for each model
lasso_auc <- lasso_metrics %>%
  filter(.metric == "roc_auc") %>%
  dplyr::summarise(
    Mean = mean(mean),
    SE   = mean(std_err)
  ) %>%
  mutate(Model = "LASSO")

rf_auc <- rf_metrics %>%
  filter(.metric == "roc_auc") %>%
  dplyr::summarise(
    Mean = mean(mean),
    SE   = mean(std_err)
  ) %>%
  mutate(Model = "Random Forest")

xgb_auc <- xgb_metrics %>%
  filter(.metric == "roc_auc") %>%
  dplyr::summarise(
    Mean = mean(mean),
    SE   = mean(std_err)
  ) %>%
  mutate(Model = "XGBoost")


# 8) Combine into one data frame and print as a plain table
table6 <- bind_rows(lasso_auc, rf_auc, xgb_auc) %>%
  dplyr::select(Model, Mean, SE)

print(table6)

```

During 5‑fold cross‑validation, the elastic‐net model achieved the highest mean AUC (0.55±0.01), followed by random forest (0.52±0.01) and XGBoost (0.50±0.01).



{{< pagebreak >}}


# Discussion

Any additional discussion regarding the supplementary material/findings.

These papers [@mckay2020; @mckay2020a] are good examples of papers published using a fully reproducible setup similar to the one shown in this template. 

{{< pagebreak >}}


# References



